\subsection{Evaluación de etiquetadores gramaticales} 
Los etiquetadores gramaticales generalmente son evaluados comparando su precisión contra un corpus de verificación Gold Standard etiquetado por humanos. Definimos precisión como el porcentaje de todas las etiquetas en el corpus de verificación donde el etiquetador y el Gold stantard concuerdan. Los algoritmos más corrientes de etiquetado gramatical tienen una precisión alrededor del 96\%-97\% para corpus de etiquetas simples como el corpus del Penn Treebank. Estas precisiones son para palabras y puntuaciones, la precisión para palabras solas es menor.

Naturalmente uno tiende a preguntarse qué tan bueno es un 97\%. El rendimiento de un proceso de etiquetado puede ser comparado contra un límite inferior y un límite superior. Una manera de establecer un límite superior es ver que tan bien realizan la tarea los humanos. 

Marcus, por ejemplo, encontró que los etiquetadores humanos concuerdan alrededor del 96\%-97\% de las etiquetas en la versión Penn Treebank del corpus Brown. Esto sugiere que el Gold Standard debe tener un 3\%-4\% de margen de error, y por lo tanto no tiene sentido obtener una precisión del 100\%. Ratnaparkhi mostró que en las palabras donde su etiquetador ha tenido problemas de ambiguedad de etiquetación fueron exactamente las mismas en donde los humanos han etiquetado inconsistentemente en el corpus de entrenamiento. Dos experimientos por Voutilainen encontraron que cuando a los humanos se les permitió discutir etiquetas, alcanzaron un consenso en el 100\% de las etiquetas.

Por otro lado el límite inferior sugerido por Gale es elegir la etiqueta más probable con unigrama para cada palabra ambigua. La etiqueta más probable para cada palabra puede ser computada desde un corpus etiquetado a mano (que puede ser el mismo que el corpus de entrenamiento para el etiquetador que está siendo evaluado).
