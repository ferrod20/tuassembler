\subsection{Evaluación de etiquetadores gramaticales} 
Los etiquetadores gramaticales generalmente son evaluados comparando su precisión contra un corpus de verificación Gold Standard etiquetado por humanos. Por precisión nos referimos al porcentaje de todas las etiquetaas en el corpus de verificación donde el etiquetador y el Gold stantard concuerdan. Los algoritmos mas corrientes de etiquetado gramatical tienen una precisión alrededor del 96-97 para corpus de etiquetas simples como el corpus del Penn Treebank. Estas precisiones son para palabras y puntuaciones, la precisión para palabras solamente es menor.
Qué tan bueno es un 97? El rendimiento de la etiquetación puede ser comparado contra un límite inferior o piso y un límite superior o techo. Una manera de establecer un techo es ver que tan bien realizan la tarea los humanos. 

Marcus, por ejemplo, encontró que los etiquetadores humanos concuerdan alrededor del 96-97 de las etiquetas en la versión Penn Treebank del corpus Brown. Esto sugiere que el Gold Standard debe tener un 3-4 de margen de error, y por lo tanto no tiene sentido obtener una precisión del 100. Ratnaparkhi mostró que en las palabras donde su etiquetador ha tenido problemas de ambiguedad de etiquetación fueron exactamente las mismas en donde los humanos han etiquetado inconsistentemente en el corpus de entrenamiento. Dos experimientos por Voutilainen encontraron que cuando a los humanos se les permitió discutir etiquetas, alcanzaron un consenso en el 100 de las etiquetas.

El piso standard sugerido por Gale es elegir la etiqueta más probable con unigrama para cada palabra ambigua. La etiqueta más probable para cada palabra puede ser computada desde un corpus etiquetado a mano (que puede ser el mismo que el corpus de entrenamiento para el etiquetador que está siendo evaluado).
