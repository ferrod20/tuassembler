\section{Evaluación de etiquetadores gramaticales} 
Los etiquetadores gramaticales generalmente son evaluados comparando su precisión contra un corpus de verificación\footnote{También llamado \textsl{Gold Standard}} etiquetado por humanos. Definimos precisión como el porcentaje de todas las etiquetas en el corpus de verificación donde el etiquetador y el \textsl{Gold Standard} concuerdan. Los algoritmos actuales de etiquetado gramatical tienen una precisión del 96\%-97\% para conjuntos de etiquetas simples como el \textsl{Penn Treebank}. Estas precisiones son para palabras y puntuaciones, la precisión para palabras solas es menor.

Naturalmente uno tiende a preguntarse qué tan bueno es un 97\%. El rendimiento de un proceso de etiquetado puede ser comparado contra un límite inferior y un límite superior. Una manera de establecer un límite superior es ver que tan bien realizan la tarea los humanos. 

\textsl{Marcus}, por ejemplo, encontró que los etiquetadores humanos concuerdan en el 96\%-97\% de las etiquetas en el corpus \textsl{Brown} etiquetado con etiquetas \textsl{Penn Treebank}. Esto sugiere que el \textsl{Gold Standard} debe tener un 3\%-4\% de margen de error, y por lo tanto no tiene sentido obtener una precisión del 100\%. \textsl{Ratnaparkhi} mostró que en las palabras donde su etiquetador ha tenido problemas de ambigüedad de etiquetación fueron exactamente las mismas en donde los humanos han etiquetado inconsistentemente el corpus de entrenamiento. Dos experimientos realizados por \textsl{Voutilainen} encontraron que cuando a los humanos se les permitió discutir etiquetas, alcanzaron un consenso en el 100\% de las etiquetas.

Por otro lado el límite inferior sugerido por \textsl{Gale} es elegir la etiqueta más probable aplicando el modelo de unigrama para cada palabra ambigüa. La etiqueta más probable para cada palabra puede ser computada desde un corpus etiquetado a mano (que puede ser el mismo que el corpus de entrenamiento para el etiquetador que está siendo evaluado).
