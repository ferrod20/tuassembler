\section{Palabras desconocidas} 
Todos los algoritmos de etiquetado gramatical presentados anteriormente requieren un diccionario que liste las posibles etiquetas de cada palabra para que posteriormente el proceso de etiquetado se encargue de identificar la etiqueta correcta. Pero claro, hay un problema: ningún diccionario, derivado o no de un corpus, es capaz de contener todas las palabras. Los sustantivos propios y los acrónimos son creados muy frecuentemente, de hecho ingresan al lenguaje nuevos sustantivos comunes y verbos en una proporción sorprendente. Por lo tanto, para construir un etiquetador completo no podemos utilizar siempre un diccionario para obtener $P(w_i|t_i)$. Necesitamos algún método para adivinar la etiqueta de una palabra desconocida.

El algoritmo más básico para manejar palabras desconocidas es suponer que cada palabra desconocida es ambigüa entre todas las posibles etiquetas, con igual probabilidad. Entonces el etiquetador debe confiar únicamente en etiquetas contextuales para sugerir la etiqueta adecuada. Un algoritmo ligeramente más complejo está basado en la idea de que la distribución de probabilidad de las etiquetas sobre las palabras desconocidas es muy similar a la distribución de las etiquetas sobre palabras que ocurren solo una vez en un corpus de entrenamiento, una idea sugerida por \emph{Baayen y Sproat (1996)} y \emph{Dermatas y Kokkinakis (1995)}. Estas palabras que ocurren solo una vez son conocidas como \emph{hapax legomena}.

Por ejemplo, las palabras desconocidas y \emph{hapax legomena} son similares en el hecho de que son más probables de ser sustantivos, seguidas por verbos, pero infrecuentemente suelen ser determinantes. Entonces la probabilidad $P(w_i|t_i)$ para una palabra desconocida es determinada por el promedio de la distribución sobre todos los conjuntos de  palabras de una sola ocurrencia en el corpus de entrenamiento. En resúmen, la idea es utilizar ``cosas que hemos visto una vez'' como un estimador para ``cosas que nunca hemos visto''.

De todas maneras, la mayoría de los algoritmos para palabras desconocidas hace uso de una fuente de información mucho más poderosa: la morfología de las palabras. Para el inglés, por ejemplo, palabras terminadas en \emph{s} tienden a ser sustantivos plurales (NNS), palabras terminadas en \emph{ed} tienden a ser pasado participio (VBN), palabras terminadas en \emph{able} tienden a ser adjetivos (JJ), y así. Incluso si nunca vimos una palabra, podemos utilizar hechos sobre su forma morfológica para adivinar su etiqueta. Además la información ortográfica puede ser de mucha ayuda. Por ejemplo, palabras que comienzan con letras mayúsculas generalmente son sustantivos propios (NNP). La presencia de un guión es también una característica útil; las palabras con guión tienen más probabilidad de ser adjetivos (JJ).

¿Cómo son combinadas y utilizadas estas características en los etiquetadores gramaticales? Un método es entrenar por separado estimadores de probabilidad para cada característica, asumiendo independencia, y multiplicando las probabilidades. 
\emph{Weischedel (1993)} construyó un modelo así, basado en cuatro clases específicas. Utilizaron 3 terminaciones infleccionales (\emph{ed}, \emph{s}, \emph{ing}), 32 terminaciones derivacionales (como \emph{ion}, \emph{al}, \emph{ive} y \emph{ly}), 4 valores de mayúscula dependiendo si una palabra es inicio de oración (+/- mayúscula, +/- inicio) y donde una palabra fué guionada. Para cada característica, entrenaron estimadores de máxima verosimilitud de la característica dada una etiqueta desde un corpus de entrenamiento etiquetado. Entonces combinaron las características para estimar la probabilidad de una palabra desconocida asumiendo independencia y multiplicando:
\begin{equation}
P(w_i|t_i)=p(\textrm{palabra desconocida}|t_i)p(\textrm{mayúscula}|t_i)p(\textrm{final/guión}|t_i)
\end{equation}

Otro acercamiento basado en HMM, proveniente del trabajo realizado por \emph{Samuelsson (1993)} y \emph{Brants (2000)}, generaliza el uso de morfología en una manera basada en datos. En este acercamiento, en vez de preseleccionar ciertos sufijos a mano, son consideradas todas las secuencias finales de letras de todas las palabras. Consideran sufijos menores a diez letras, computando para cada sufijo de longitud $i$ la probabilidad de la etiqueta $t_i$:
\begin{equation}
P(t_i|l_{n-i+1},\ldots,l_n)
\end{equation}

Estas probabilidades son suavizadas utilizando sucesivamente menores y menores sufijos. Esta información de sufijos se mantiene por separado para palabras en mayúscula y minúscula. 

En general, la mayoría de los modelos de palabras desconocidas intentan capturar el hecho de que las palabras desconocidas improbablemente pertenecen a clases cerradas de palabras. \emph{Brants} modela este hecho computando solamente las probabilidades de sufijos desde el corpus de entrenamiento para palabras cuya frecuencia en el corpus de enternamiento es $\leq$ 10.